{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92504e4-4e91-4707-b0be-33703f78459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8595fd8-5716-45f2-9cba-e69b0593c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv(\"Twitter Hate Speech - Twitter Hate Speech.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5903a674-6729-45b3-b40a-10a2acf07933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so...</td>\n",
       "      <td>when a father is dysfunctional and is so self...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit i cant use cause they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>0</td>\n",
       "      <td>ate @user isz that youuu?ðððððð...</td>\n",
       "      <td>ate  isz that youuuðððððððððâï</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>0</td>\n",
       "      <td>to see nina turner on the airwaves trying to w...</td>\n",
       "      <td>to see nina turner on the airwaves trying to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>0</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "      <td>sikh temple vandalised in in calgary wso cond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "      <td>thank you  for you follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "0          1      0  @user when a father is dysfunctional and is so...   \n",
       "1          2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2          3      0                                bihday your majesty   \n",
       "3          4      0  #model   i love u take with u all the time in ...   \n",
       "4          5      0             factsguide: society now    #motivation   \n",
       "...      ...    ...                                                ...   \n",
       "31957  31958      0  ate @user isz that youuu?ðððððð...   \n",
       "31958  31959      0  to see nina turner on the airwaves trying to w...   \n",
       "31959  31960      0  listening to sad songs on a monday morning otw...   \n",
       "31960  31961      1  @user #sikh #temple vandalised in in #calgary,...   \n",
       "31961  31962      0                     thank you @user for you follow   \n",
       "\n",
       "                                           cleaned_tweet  \n",
       "0       when a father is dysfunctional and is so self...  \n",
       "1        thanks for lyft credit i cant use cause they...  \n",
       "2                                    bihday your majesty  \n",
       "3      model   i love u take with u all the time in u...  \n",
       "4                   factsguide society now    motivation  \n",
       "...                                                  ...  \n",
       "31957                     ate  isz that youuuðððððððððâï  \n",
       "31958  to see nina turner on the airwaves trying to w...  \n",
       "31959  listening to sad songs on a monday morning otw...  \n",
       "31960   sikh temple vandalised in in calgary wso cond...  \n",
       "31961                          thank you  for you follow  \n",
       "\n",
       "[31962 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993c44f-df8e-462e-b4e9-0d06c10746ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41101afe-1e94-4aa7-ad70-21134eeefc93",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning with Regex and Python\n",
    "\n",
    "For cleaning the tweets, We can use regular expressions (regex) to identify and remove or replace parts of the tweets that aren't useful for your analysis. Common steps include:\n",
    "\n",
    "- **Removing user mentions** (e.g., `@user`)\n",
    "- **Removing URLs**\n",
    "- **Removing special characters and emoticons**\n",
    "- **Optionally, converting all text to lowercase** to ensure uniformity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa874bd8-2c88-4544-9684-6b856def6492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                      cleaned_tweet\n",
      "0   1      0   when a father is dysfunctional and is so self...\n",
      "1   2      0    thanks for lyft credit i cant use cause they...\n",
      "2   3      0                                bihday your majesty\n",
      "3   4      0  model   i love u take with u all the time in u...\n",
      "4   5      0               factsguide society now    motivation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to clean tweets\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'@user', '', tweet)  # Remove @user\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)  # Remove URLs\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)  # Remove mentions and hashtags\n",
    "    tweet = re.sub(r'\\\\x[\\w]{2,}', '', tweet)  # Remove encoded characters (e.g., emoji)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)  # Remove punctuation\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    return tweet\n",
    "\n",
    "# Apply cleaning function to tweets\n",
    "Data['cleaned_tweet'] = Data['tweet'].apply(clean_tweet)\n",
    "\n",
    "# Show cleaned data\n",
    "print(Data[['id', 'label', 'cleaned_tweet']].head())\n",
    "\n",
    "# Display the counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbfbf2-fe7d-423a-b018-a3461d2d351f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "115c10f1-2f56-47dc-b4ed-bf465e048dc3",
   "metadata": {},
   "source": [
    "## Featurization in Machine Learning and NLP\r\n",
    "\r\n",
    "Featurization is a critical process in machine learning and natural language processing (NLP), involving the transformation of raw data into a structured format that models can understand and process effectively. This process enables the models to learn from the data and make predictions or classifications based on it.\r\n",
    "\r\n",
    "### Traditional Machine Learning Models\r\n",
    "\r\n",
    "For traditional machine learning models, featurization typically involves creating vectors of numbers that represent the data. These vectors are constructed through various techniques, each designed to capture different aspects of the data's structure and meaning:\r\n",
    "\r\n",
    "- **Bag of Words (BoW)**: This technique represents text data as a bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity. It involves counting the occurrence of words within a document, which results in a sparse matrix representation where each row corresponds to a document and each column to a word in the dataset's vocabulary.\r\n",
    "\r\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: TF-IDF goes a step further than BoW by considering not only the frequency of words in a single document but also how unique these words are across all documents in the corpus. It aims to highlight words that are frequent in a document but not common in the entire dataset, providing a more nuanced representation of the text data.\r\n",
    "\r\n",
    "### Transformers and BertTokenizer\r\n",
    "\r\n",
    "With the advent of transformer models, featurization has taken on new dimensions. In the context of transformers, featurization is handled through sophisticated tokenization and encoding steps, primarily facilitated by tools like the `BertTokenizer`. This involves several key processes:\r\n",
    "\r\n",
    "- **Tokenization**: Raw text is split into tokens (words or subwords), which are then mapped to numerical IDs from the model's predefined vocabulary. This step converts text into a sequence of numbers, making it computationally tractable for the model.\r\n",
    "\r\n",
    "- **Encoding**: Additional steps, such as adding special tokens (`[CLS]`, `[SEP]`, `[PAD]`), padding sequences to a fixed length, and creating attention masks, are performed. These processes ensure that the model can correctly interpret the structure and content of the input data, focusing on meaningful tokens and ignoring padding when necessary.\r\n",
    "\r\n",
    "This approach to featurization leverages the inherent capabilities of transformer models to understand and process text data deeply, capturing contextual relationships and nuances that were challenging to represent with traditional techniques.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcb0ebee-71dd-4220-9ebe-6923917e76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomTweetDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, tokenizer, max_token_len=128):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tweet = self.tweets[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare the dataset\n",
    "tweets = Data.cleaned_tweet.to_numpy()\n",
    "labels = Data.label.to_numpy()\n",
    "dataset = CustomTweetDataset(tweets=tweets, labels=labels, tokenizer=tokenizer)\n",
    "\n",
    "# Create a DataLoader\n",
    "# Create a DataLoader with num_workers set to 0 for troubleshooting\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07ecef-bbd3-40fd-9f73-2c59665e30dc",
   "metadata": {},
   "source": [
    "### Preparing a Dataset for BERT Model Training\n",
    "\n",
    "The code snippet demonstrates how to prepare a custom dataset for training a BERT model using PyTorch and the Hugging Face `transformers` library. Here's a step-by-step explanation:\n",
    "\n",
    "#### Importing Libraries\n",
    "\n",
    "- `BertTokenizer`: From Hugging Face's `transformers`, used to tokenize text into a format BERT understands.\n",
    "- `Dataset`, `DataLoader`: From `torch.utils.data`, facilitate custom data handling and batching for training.\n",
    "- `torch`: The main PyTorch library.\n",
    "\n",
    "#### CustomTweetDataset Class\n",
    "\n",
    "- **Purpose**: Extends the `Dataset` class to handle the specifics of our text data (tweets).\n",
    "- **Initialization Parameters**:\n",
    "  - `tweets`: Array of tweet texts.\n",
    "  - `labels`: Array of labels corresponding to each tweet.\n",
    "  - `tokenizer`: Instance of `BertTokenizer`.\n",
    "  - `max_token_len`: Maximum length for tokenized tweet sequences.\n",
    "- **Methods**:\n",
    "  - `__len__`: Returns the total number of tweets in the dataset.\n",
    "  - `__getitem__`: Fetches a single processed item from the dataset by index. It tokenizes the tweet, applies padding and truncation, and converts it to tensors.\n",
    "    - **Encoding**:\n",
    "      - `add_special_tokens`: Adds tokens like [CLS] and [SEP] necessary for BERT.\n",
    "      - `max_length`: Ensures all sequences are of the same length.\n",
    "      - `return_token_type_ids`: Omits token type ids (not needed for sequence classification).\n",
    "      - `padding` and `truncation`: Ensures uniform sequence length.\n",
    "      - `return_attention_mask`: Generates a mask to distinguish real tokens from padding.\n",
    "      - `return_tensors='pt'`: Returns PyTorch tensors.\n",
    "    - **Return Value**: A dictionary with `input_ids`, `attention_mask`, and `labels` for the given tweet.\n",
    "\n",
    "#### Tokenizer Initialization\n",
    "\n",
    "- **BERT Tokenizer**: Loaded with `from_pretrained('bert-base-uncased')`, preparing it for processing English text in lowercase.\n",
    "\n",
    "#### Dataset Preparation\n",
    "\n",
    "- Converts tweet texts and labels from the DataFrame `Data` into numpy arrays for processing.\n",
    "- Initializes the `CustomTweetDataset` with these arrays and the tokenizer.\n",
    "\n",
    "#### DataLoader Creation\n",
    "\n",
    "- Wraps the dataset in a `DataLoader` for efficient batch processing during model training.\n",
    "- `batch_size=32`: Determines how many items are processed together as a batch.\n",
    "- `num_workers=0`: For troubleshooting, this setting avoids parallel data loading to simplify debugging.\n",
    "\n",
    "This setup is essential for preparing text data for training with BERT, ensuring each step from tokenization to batch loading is optimized for performance and compatibility with the model's requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6c6bf8-6ab6-4d93-8f02-b7849f3932ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128]) torch.Size([32, 128]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch[\"input_ids\"].shape, batch[\"attention_mask\"].shape, batch[\"labels\"].shape)\n",
    "    break  # This should exit after the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac31e3f-3318-45b4-bbf8-8ef277113c3a",
   "metadata": {},
   "source": [
    "### Understanding DataLoader Output\n",
    "\n",
    "The output from the DataLoader indicates successful batch preparation. Let's dissect the output for a comprehensive understanding:\n",
    "\n",
    "#### 1. **`torch.Size([32, 128])` for `input_ids`:**\n",
    "   - **Batch Size**: 32 tweets per batch.\n",
    "   - **Sequence Length**: Each tweet is tokenized into a sequence of 128 tokens.\n",
    "   - **Content**: Includes both the actual words/subwords (BERT utilizes a subword tokenizer) and any padding to achieve the uniform length of 128 tokens.\n",
    "\n",
    "#### 2. **`torch.Size([32, 128])` for `attention_mask`:**\n",
    "   - **Batch Size and Sequence Length**: Mirrors the `input_ids`, with 32 attention masks, each 128 tokens in length.\n",
    "   - **Function**: Indicates which tokens should be considered by the model (1's for words, including padding, and 0's for padding tokens to be ignored).\n",
    "\n",
    "#### 3. **`torch.Size([32])` for `labels`:**\n",
    "   - **Batch Size**: Corresponds to the 32 labels in each batch, one for each tweet.\n",
    "   - **Purpose**: Used by the model to learn during training, indicating whether a tweet is classified as hate speech or not, among other potential classes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
